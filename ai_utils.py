import re
import json
import os
from datetime import datetime, timezone, timedelta
import time
import threading
import hashlib
from pathlib import Path
from dotenv import load_dotenv
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type
from config import logger
from constants import (
    BUYER_TRIGGERS,
    SELLER_TERMS,
    OFFER_TERMS,
    REALTY_HINT_TERMS,
    QUESTION_INDICATORS,
    SALESY_TERMS,
    PROMO_CTA_TERMS,
)
# Load environment variables
load_dotenv()
try:
    from openai import OpenAI, RateLimitError, APIError, Timeout
    from openai._types import NOT_GIVEN
except ImportError:
    import openai
    from openai import OpenAI  # –∫–ª–∏–µ–Ω—Ç –µ—Å—Ç—å
    # shim: –µ—Å–ª–∏ –≤ –≤–µ—Ä—Å–∏–∏ openai –Ω–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤, –ø–æ–¥–º–µ–Ω—è–µ–º Exception
    RateLimitError = getattr(openai, "RateLimitError", Exception)
    APIError = getattr(openai, "APIError", Exception)
    Timeout = getattr(openai, "Timeout", Exception)
    NOT_GIVEN = object()
DEBUG_PROMPT_TRACE = False


def _coerce_usage_dict(usage_obj):
    if usage_obj in (None, NOT_GIVEN):
        return None
    # If object has a direct usage attribute, inspect that first
    maybe_usage = getattr(usage_obj, 'usage', None) if not isinstance(usage_obj, dict) else None
    if maybe_usage is not None and maybe_usage is not usage_obj:
        converted = _coerce_usage_dict(maybe_usage)
        if converted is not None:
            return converted
    if isinstance(usage_obj, dict):
        return usage_obj
    if hasattr(usage_obj, 'to_dict'):
        try:
            data = usage_obj.to_dict()
            if isinstance(data, dict):
                # Responses API exposes usage nested under 'usage' key
                if 'usage' in data and isinstance(data['usage'], dict):
                    return data['usage']
                return data if 'input_tokens' in data else None
        except Exception:
            pass
    if hasattr(usage_obj, '__dict__'):
        try:
            data = dict(usage_obj.__dict__)
            if 'usage' in data and isinstance(data['usage'], dict):
                return data['usage']
            return data if 'input_tokens' in data else None
        except Exception:
            pass
    return None


def _log_usage(model_name: str, usage_obj, resp_id=None):
    usage_dict = _coerce_usage_dict(usage_obj)
    if usage_dict is None:
        return
    input_tokens = usage_dict.get('input_tokens')
    if input_tokens is None:
        input_tokens = usage_dict.get('prompt_tokens')
    output_tokens = usage_dict.get('output_tokens')
    if output_tokens is None:
        output_tokens = usage_dict.get('completion_tokens')
    total_tokens = usage_dict.get('total_tokens')
    if total_tokens is None and input_tokens is not None and output_tokens is not None:
        total_tokens = input_tokens + output_tokens
    cached_tokens = None
    input_details = usage_dict.get('input_tokens_details') or usage_dict.get('prompt_tokens_details') or {}
    if isinstance(input_details, dict):
        cached_tokens = input_details.get('cached_tokens')
    ts = datetime.now().strftime('%m-%d %H:%M:%S')
    try:
        with AI_TOKENS_LOG_PATH.open('a', encoding='utf-8') as tf:
            tf.write(f"{ts} | model={model_name} | in={input_tokens} cached_in={cached_tokens} out={output_tokens} total={total_tokens} resp_id={resp_id}\n")
    except Exception:
        pass
BASE_DIR = Path(__file__).resolve().parent
AI_TOKENS_LOG_PATH = BASE_DIR / "ai_tokens.log"
import snowballstemmer
_ru_stemmer = snowballstemmer.stemmer('russian')


def _stem(word: str) -> str:
    """Return lowercase snowball stem for Russian word."""
    return _ru_stemmer.stemWord(word.lower())
CATEGORY_ALIASES = {
    "—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞": "—Å—Ç—Ä–∞—Ö–æ–≤–∫–∏",
    "—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ": "—Å—Ç—Ä–∞—Ö–æ–≤–∫–∏",
}
try:
    _prompt_cats_path = BASE_DIR / "categories.json"
    with _prompt_cats_path.open(encoding="utf-8") as cf:
        _PROMPT_CATEGORIES = sorted(json.load(cf).keys())
except Exception:
    _PROMPT_CATEGORIES = ["–∞—Ä–µ–Ω–¥–∞ –∞–≤—Ç–æ", "–∞—Ä–µ–Ω–¥–∞ —è—Ö—Ç", "–±—å—é—Ç–∏", "–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å", "—Å—Ç—Ä–∞—Ö–æ–≤–∫–∏", "—Ç—Ä–∞–Ω—Å—Ñ–µ—Ä", "—ç–∫—Å–∫—É—Ä—Å–∏–∏"]
PROMPT_CATEGORY_CONTEXT = "–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: " + ", ".join(_PROMPT_CATEGORIES)
CONF_THRESHOLD = 0.79  # confidence threshold for auto-accepting leads (deliver)
# –ü—Ä–æ—Å—Ç–æ–π —Ä—É—á–Ω–æ–π –∫—ç—à, –ø–æ—Ç–æ–º—É —á—Ç–æ —Å–ø–∏—Å–∫–∏ (list) –Ω–µ —Ö–µ—à–∏—Ä—É–µ–º—ã –¥–ª—è lru_cache
_classify_cache = {}
_CLASSIFY_CACHE_MAXSIZE = 5000
# TTL for cache entries (seconds), default 12 hours
_CLASSIFY_CACHE_TTL = int(float(os.getenv("AI_CACHE_TTL", str(12 * 3600))))

# --- Rate‚Äëlimit settings ---
_RATE_LIMIT_RPS = float(os.getenv("OPENAI_RPS", "3"))  # –º–∞–∫—Å. –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
_MIN_INTERVAL = 1.0 / _RATE_LIMIT_RPS
_last_call_ts = 0.0
_rate_lock = threading.Lock()
# Helper for rate-limit


def _apply_rate_limit():
    """–ë–ª–æ–∫–∏—Ä—É–µ—Ç –ø–æ—Ç–æ–∫, —á—Ç–æ–±—ã –≤—ã–¥–µ—Ä–∂–∞—Ç—å –∑–∞–¥–∞–Ω–Ω—ã–π RPS."""
    global _last_call_ts
    with _rate_lock:
        now = time.time()
        wait_sec = _MIN_INTERVAL - (now - _last_call_ts)
        if wait_sec > 0:
            time.sleep(wait_sec)
        _last_call_ts = time.time()

# --- Robust JSON parsing for AI output --------------------------------------
import itertools


def _try_parse_ai_json(ai_text: str):
    """Best-effort parse of a JSON object from model output.
    Returns dict or None.
    """
    if not ai_text:
        return None
    # 1) Fast path
    try:
        obj = json.loads(ai_text)
        return obj if isinstance(obj, dict) else None
    except Exception:
        pass
    # 2) Extract the first {...} block
    import re as _re
    m = _re.search(r"\{[\s\S]*\}", ai_text)
    if m:
        snippet = m.group(0)
        # quick fixes: single quotes -> double, trailing commas
        fixed = snippet.replace("'", '"')
        fixed = _re.sub(r",\s*([}\]])", r"\1", fixed)
        try:
            obj = json.loads(fixed)
            return obj if isinstance(obj, dict) else None
        except Exception:
            pass
    return None
# –û–±—ë—Ä—Ç–∫–∞ —Å retry –¥–ª—è Responses API
@retry(
    wait=wait_exponential(min=1, max=30, multiplier=2),
    stop=stop_after_attempt(4),
    retry=retry_if_exception_type(Exception)
)


def _responses_create_with_retry(client: OpenAI, **kwargs):
    """–í—ã–∑–æ–≤ responses.create —Å rate‚Äëlimit –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º back‚Äëoff."""
    _apply_rate_limit()
    return client.responses.create(**kwargs)
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ OpenAI (–ª–µ–Ω–∏–≤–æ, –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω)


def get_openai_client():
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY is not set in .env")
    return OpenAI(api_key=api_key)
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã
_contact_regex = re.compile(r"(@[\w_]+|t\.me/[\w_\-]+|https?://t\.me/[\w_\-]+|\+?[\d\-\s\(\)]{7,}|whatsapp)", flags=re.IGNORECASE)
_PRICE_REGEX = re.compile(
    r"\b\d+(?:[\.,]\d+)?\s?(?:‚Ç¨|eur|–µ–≤—Ä–æ|usd|\$|–¥–æ–ª–ª–∞—Ä|‚Ç∫|try)(?:\b|[/\-]?[–∞-—èa-z]{0,8})",
    flags=re.IGNORECASE
)


def contains_contact(s: str) -> bool:
    return bool(_contact_regex.search(s))


def classify_relevance(text: str, categories: list[str], client_ai=None) -> dict:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: —Ç–µ–ø–µ—Ä—å –Ω–µ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è (—Å–ª–∏—è–Ω–∏–µ –≤ –æ–¥–∏–Ω –ò–ò‚Äë—á–µ–∫)."""
    return {"relevant": True, "explanation": "merged-into-main"}


def _sanitize_result(result: dict) -> dict:
    # Ensure keys exist with defaults
    relevant = result.get("relevant", False)
    category = result.get("category")
    subcategory = result.get("subcategory")
    region = result.get("region")
    explanation = result.get("explanation", "")
    confidence = result.get("confidence", 0.0)
    # Coerce confidence to float and clamp between 0 and 1
    try:
        confidence = float(confidence)
    except Exception:
        confidence = 0.0
    confidence = max(0.0, min(1.0, confidence))
    # Truncate explanation to 70 chars with ellipsis if needed
    if len(explanation) > 70:
        explanation = explanation[:67] + "..."
    # Determine accepted and borderline flags
    accepted = relevant is True and confidence >= CONF_THRESHOLD
    borderline = False
    if relevant is True and confidence < CONF_THRESHOLD:
        borderline = True
    # Update result dict
    result["relevant"] = relevant
    result["category"] = category
    result["subcategory"] = subcategory
    result["region"] = region
    result["explanation"] = explanation
    result["confidence"] = confidence
    result["accepted"] = accepted
    if borderline:
        result["borderline"] = True
    elif "borderline" in result:
        del result["borderline"]
    return result


def calibrate_confidence(raw_confidence: float) -> float:
    """Piecewise calibration of confidence based on observed buckets."""
    if raw_confidence >= 0.9:
        calibrated = 0.92
    elif 0.8 <= raw_confidence < 0.9:
        calibrated = 0.85
    elif 0.6 <= raw_confidence < 0.8:
        calibrated = 0.7
    elif 0.5 <= raw_confidence < 0.6:
        calibrated = 0.55
    else:
        calibrated = raw_confidence
    return max(0.0, min(1.0, calibrated))


def classify_text_with_ai(text: str,
                          categories: list,
                          locations: list,
                          client_ai=None) -> dict:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å keys: relevant, category, subcategory, region, explanation, confidence.
    –ú–æ–∂–µ—Ç –¥–æ–±–∞–≤–ª—è—Ç—å override_reason.
    """
    if client_ai is None:
        client_ai = get_openai_client()
    # –ö–ª—é—á –¥–ª—è –∫—ç—à–∞: —Ö—ç—à —Ç–µ–∫—Å—Ç–∞ + —Ö—ç—à –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    text_hash = hashlib.sha1(text.encode('utf-8')).hexdigest()
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ–¥–∏–Ω —Ä–∞–∑, —á—Ç–æ–±—ã —Ö—ç—à –∏ –ø—Ä–æ–º–ø—Ç –±—ã–ª–∏ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏
    raw_cats = list(categories) if categories else []
    cat_subset_sorted = sorted(dict.fromkeys(raw_cats))
    cats_hash = hashlib.sha1(str(cat_subset_sorted).encode('utf-8')).hexdigest()
    key = f"{text_hash}_{cats_hash}"
    if key in _classify_cache:
        entry = _classify_cache[key]
        cached_ts = entry.get('ts') if isinstance(entry, dict) else None
        cached_payload = entry.get('payload') if isinstance(entry, dict) else None
        if cached_ts is not None and (time.time() - cached_ts) <= _CLASSIFY_CACHE_TTL and isinstance(cached_payload, dict):
            return cached_payload.copy()
        else:
            # Expired entry
            try:
                del _classify_cache[key]
            except KeyError:
                pass
    # –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Å–ø–∏—Å–∫–∏ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    focus_category_list = ', '.join(f'"{cat}"' for cat in cat_subset_sorted) if cat_subset_sorted else ''
    category_list = ', '.join(f'"{cat}"' for cat in _PROMPT_CATEGORIES) if _PROMPT_CATEGORIES else ''
    # –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–†–û–ú–ü–¢
    system_prompt = f"""–í—ã —è–≤–ª—è–µ—Ç–µ—Å—å —ç–∫—Å–ø–µ—Ä—Ç–æ–º –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ (–ª–∏–¥–æ–≤) –≤ Telegram-–≥—Ä—É–ø–ø–∞—Ö. –í–∞—à–∞ –∑–∞–¥–∞—á–∞ - —Ç–æ—á–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º –ª–∏–¥–æ–º, –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ.
–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω—ã –≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ –Ω–∏–∂–µ. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏—Ö –∏ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –æ—Ç–≤–µ—Ç—å—Ç–µ –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ –±–µ–∑ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤:
{{"relevant": true/false, "category": "–æ–¥–Ω–∞ –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"|null, "subcategory": "–ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è"|null, "region": "—Ä–µ–≥–∏–æ–Ω"|null, "explanation": "–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ", "confidence": 0.0-1.0}}
–ö—Ä–∏—Ç–µ—Ä–∏–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –ª–∏–¥–∞:
1. –°–æ–æ–±—â–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –ø—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –Ω–∞ —É—Å–ª—É–≥—É –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
2. –°–æ–æ–±—â–µ–Ω–∏–µ –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ—Å–≤–µ–Ω–Ω—ã–µ —É–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å –≤ —É—Å–ª—É–≥–µ
3. –°–æ–æ–±—â–µ–Ω–∏–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤–æ–ø—Ä–æ—Å–æ–º –∏–ª–∏ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ–º –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏
4. –ó–∞–ø—Ä–æ—Å —Ü–µ–Ω—ã –∏–ª–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ —É—Å–ª—É–≥–∏ —Ç–∞–∫–∂–µ —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º –ª–∏–¥–æ–º
5. –ó–∞–ø—Ä–æ—Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —É—Å–ª—É–≥–∏ —Ç–∞–∫–∂–µ —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º –ª–∏–¥–æ–º
6. –§—Ä–∞–∑—ã –≤—Ä–æ–¥–µ "—Å–Ω–∏–º—É", "–∏—â—É –∫–≤–∞—Ä—Ç–∏—Ä—É" ‚Äî —ç—Ç–æ –∑–∞–ø—Ä–æ—Å—ã, –¥–∞–∂–µ –µ—Å–ª–∏ –∑–≤—É—á–∞—Ç –∫–∞–∫ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
7. –ö–∞—Ç–µ–≥–æ—Ä–∏—è "–¢—Ä–∞–Ω—Å—Ñ–µ—Ä" ‚Äî —Ç–æ–ª—å–∫–æ –ø—Ä–æ –ø–µ—Ä–µ–≤–æ–∑–∫—É –ª—é–¥–µ–π, –ø–µ—Ä–µ–≤–æ–∑–∫–∞ –≤–µ—â–µ–π –Ω–µ—Ä–µ–ª–≤–∞–Ω—Ç–Ω–æ.
–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ù–ï—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (relevant: false):
1. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —É—Å–ª—É–≥ (–Ω–µ –∑–∞–ø—Ä–æ—Å—ã) - –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç —É—Å–ª—É–≥—É, –∞ –Ω–µ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –µ—ë (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–∏—â—É –ø–æ–ø—É—Ç—á–∏–∫–æ–≤")
2. –û—Ç–∑—ã–≤—ã –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –æ –ø—Ä–æ—à–ª–æ–º –æ–ø—ã—Ç–µ
3. –û–±—ä—è–≤–ª–µ–Ω–∏—è –æ –ø—Ä–æ–¥–∞–∂–µ/–∞—Ä–µ–Ω–¥–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏, –∞–≤—Ç–æ), –µ—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
4. –°–ø–∞–º –∏ —Ä–µ–∫–ª–∞–º–∞
5. –°–æ–æ–±—â–µ–Ω–∏—è –±–µ–∑ —è–≤–Ω–æ–π –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏ –≤ —É—Å–ª—É–≥–µ
6. –§—Ä–∞–∑—ã –≤–∏–¥–∞ "–∞—Ä–µ–Ω–¥–∞/–ø—Ä–æ–∫–∞—Ç <—É—Å–ª—É–≥–∞> –æ—Ç <—Ü–µ–Ω–∞>" (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–∞—Ä–µ–Ω–¥–∞ —è—Ö—Ç—ã –æ—Ç 200‚Ç¨") ‚Äî —ç—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —É—Å–ª—É–≥–∏, –Ω–µ –∑–∞–ø—Ä–æ—Å.
7. –ü–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤, CTA ‚Äú–Ω–∞–ø–∏—à–∏/–∑–∞–±—Ä–æ–Ω–∏—Ä—É–π‚Äù, —Å–ª–æ–≤–∞ ‚Äú—Å–≤–æ–±–æ–¥–Ω–∞‚Äù, ‚Äú–¥–æ—Å—Ç—É–ø–µ–Ω‚Äù, ‚Äú–±—Ä–æ–Ω—å‚Äù, ‚Äú–∑–∞–µ–∑–¥‚Äù, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å–ª–æ–≤ ‚Äú–∏—â—É/–Ω—É–∂–µ–Ω/—Å–∫–æ–ª—å–∫–æ‚Äù ‚Äî —ç—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –∞ –Ω–µ –ª–∏–¥!
–£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (confidence):
- 0.9-1.0: –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - —è–≤–Ω—ã–π –∑–∞–ø—Ä–æ—Å
- 0.7-0.9: –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - –≤–µ—Ä–æ—è—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å
- 0.5-0.7: –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - –≤–æ–∑–º–æ–∂–Ω—ã–π –∑–∞–ø—Ä–æ—Å
- 0.0-0.5: –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å
–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º (–¥–æ 70 —Å–∏–º–≤–æ–ª–æ–≤), —É–∫–∞–∑—ã–≤–∞—è –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–ª–∏ —Ñ—Ä–∞–∑—ã, –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ç–æ—Ä—ã—Ö –±—ã–ª–æ –ø—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ.
–ü—Ä–∏–º–µ—Ä—ã:
1. "–ú—ã –≤—á–µ—Ä–∞ –µ—Ö–∞–ª–∏ –∏–∑ –ö–µ–º–µ—Ä–∞ –Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–µ 4 —á–µ–ª–æ–≤–µ–∫–∞, –±–∞–≥–∞–∂ 40$" ‚Üí {{"relevant": false, "category": "—Ç—Ä–∞–Ω—Å—Ñ–µ—Ä", "subcategory": null, "region": "–ö–µ–º–µ—Ä", "explanation": "–ø—Ä–æ—à–ª—ã–π –æ–ø—ã—Ç", "confidence": 0.95}}
2. "–ü—Ä–æ–¥–∞—é –∫–≤–∞—Ä—Ç–∏—Ä—É –≤ –°—Ç–∞–º–±—É–ª–µ, 2+1, –µ–≤—Ä–æ—Ä–µ–º–æ–Ω—Ç" ‚Üí {{"relevant": false, "category": null, "subcategory": null, "region": null, "explanation": "–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∂–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏", "confidence": 0.98}}
3. "–ö—Ç–æ-–Ω–∏–±—É–¥—å –ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è —É—Å–ª—É–≥–∞–º–∏ –∫–ª–∏–Ω–∏–Ω–≥–∞? –ü–æ—Å–æ–≤–µ—Ç—É–π—Ç–µ" ‚Üí {{"relevant": true, "category": "–∫–ª–∏–Ω–∏–Ω–≥", "subcategory": null, "region": null, "explanation": "–∑–∞–ø—Ä–æ—Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –∫–ª–∏–Ω–∏–Ω–≥–∞", "confidence": 0.85}}
4. "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä –∏–∑ –∞—ç—Ä–æ–ø–æ—Ä—Ç–∞ –¥–æ –æ—Ç–µ–ª—è –≤ –°–∏–¥–µ?" ‚Üí {{"relevant": true, "category": "—Ç—Ä–∞–Ω—Å—Ñ–µ—Ä", "subcategory": null, "region": "–°–∏–¥–µ", "explanation": "–∑–∞–ø—Ä–æ—Å —Ü–µ–Ω—ã –Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä", "confidence": 0.90}}
5. "–ö–∞–∫–æ–≤–∞ —Å—Ç–æ–∏–º–æ—Å—Ç—å —ç–∫—Å–∫—É—Ä—Å–∏–∏ –ø–æ –°—Ç–∞–º–±—É–ª—É?" ‚Üí {{"relevant": true, "category": "—ç–∫—Å–∫—É—Ä—Å–∏–∏", "subcategory": null, "region": "–°—Ç–∞–º–±—É–ª", "explanation": "–∑–∞–ø—Ä–æ—Å —Ü–µ–Ω—ã –Ω–∞ —ç–∫—Å–∫—É—Ä—Å–∏—é", "confidence": 0.90}}
6. "–ì–¥–µ –º–æ–∂–Ω–æ –∞—Ä–µ–Ω–¥–æ–≤–∞—Ç—å —è—Ö—Ç—É –≤ –ö–µ–º–µ—Ä–µ?" ‚Üí {{"relevant": true, "category": "–∞—Ä–µ–Ω–¥–∞ —è—Ö—Ç", "subcategory": null, "region": "–ö–µ–º–µ—Ä", "explanation": "–∑–∞–ø—Ä–æ—Å –∞—Ä–µ–Ω–¥—ã —è—Ö—Ç—ã", "confidence": 0.90}}
7. "–ù—É–∂–µ–Ω –º–∞—Å—Å–∞–∂ –≤ –ê–Ω—Ç–∞–ª–∏–∏, —Å–∫–æ–ª—å–∫–æ —ç—Ç–æ –±—É–¥–µ—Ç —Å—Ç–æ–∏—Ç—å?" ‚Üí {{"relevant": true, "category": "–±—å—é—Ç–∏", "subcategory": "–º–∞–Ω–∏–∫—é—Ä", "region": "–ê–Ω—Ç–∞–ª–∏—è", "explanation": "–∑–∞–ø—Ä–æ—Å —Ü–µ–Ω—ã –Ω–∞ –º–∞—Å—Å–∞–∂", "confidence": 0.90}}
8. "–°–Ω–∏–º—É –∫–≤–∞—Ä—Ç–∏—Ä—É 2+1 –≤ –ö–æ–Ω—å—è–∞–ª—Ç—ã" ‚Üí {{"relevant": true, "category": "–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å", "subcategory": "–∞—Ä–µ–Ω–¥–∞", "region": "–ö–æ–Ω—å—è–∞–ª—Ç—ã", "explanation": "–∑–∞–ø—Ä–æ—Å –∞—Ä–µ–Ω–¥—ã –∫–≤–∞—Ä—Ç–∏—Ä—ã", "confidence": 0.95}}
9. "–ò—â—É –∫–≤–∞—Ä—Ç–∏—Ä—É –≤ –ê–Ω—Ç–∞–ª–∏–∏, 2+1" ‚Üí {{"relevant": true, "category": "–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å", "subcategory": "–∞—Ä–µ–Ω–¥–∞", "region": "–ê–Ω—Ç–∞–ª–∏—è", "explanation": "–∑–∞–ø—Ä–æ—Å –∞—Ä–µ–Ω–¥—ã –∫–≤–∞—Ä—Ç–∏—Ä—ã", "confidence": 0.95}}
10. "–°–¥–∞–º –∫–≤–∞—Ä—Ç–∏—Ä—É –≤ –°–∏–¥–µ, –µ–≤—Ä–æ—Ä–µ–º–æ–Ω—Ç" ‚Üí {{"relevant": false, "category": "–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å", "subcategory": "–∞—Ä–µ–Ω–¥–∞", "region": "–°–∏–¥–µ", "explanation": "–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–¥–∞—á–∏ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏", "confidence": 0.95}}
11. "–ù—É–∂–µ–Ω —ç–ª–µ–∫—Ç—Ä–∏–∫ –≤ –ê–Ω—Ç–∞–ª–∏–∏, —Å—Ä–æ—á–Ω–æ" ‚Üí {{"relevant": true, "category": "—É—Å–ª—É–≥–∏_–º–∞—Å—Ç–µ—Ä–æ–≤", "subcategory": "—ç–ª–µ–∫—Ç—Ä–∏–∫–∞", "region": "–ê–Ω—Ç–∞–ª–∏—è", "explanation": "–∑–∞–ø—Ä–æ—Å —ç–ª–µ–∫—Ç—Ä–∏–∫–∞", "confidence": 0.95}}
12. "–ò—â—É —Å–∞–Ω—Ç–µ—Ö–Ω–∏–∫–∞ –≤ –°–∏–¥–µ, –æ–ø—ã—Ç, –≥–∞—Ä–∞–Ω—Ç–∏—è" ‚Üí {{"relevant": true, "category": "—É—Å–ª—É–≥–∏_–º–∞—Å—Ç–µ—Ä–æ–≤", "subcategory": "—Å–∞–Ω—Ç–µ—Ö–Ω–∏–∫–∞", "region": "–°–∏–¥–µ", "explanation": "–∑–∞–ø—Ä–æ—Å —Å–∞–Ω—Ç–µ—Ö–Ω–∏–∫–∞", "confidence": 0.95}}
13. ‚Äúüåä Demre ‚Äì Myra Tour üåø ‚Ä¶ –°—Ç–æ–∏–º–æ—Å—Ç—å —Ç—É—Ä–∞: 100‚Ç¨‚Ä¶ –≠—Ç–æ –∏–¥–µ–∞–ª—å–Ω—ã–π —Ç—É—Ä!‚Äù ‚Üí {{‚Äúrelevant‚Äù: false, ‚Äúcategory‚Äù: ‚Äú—ç–∫—Å–∫—É—Ä—Å–∏–∏‚Äù, ‚Äúsubcategory‚Äù: null, ‚Äúregion‚Äù: ‚Äú–î–µ–º—Ä–µ‚Äù, ‚Äúexplanation‚Äù: ‚Äú—è–≤–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Ç—É—Ä–∞‚Äù, ‚Äúconfidence‚Äù: 0.95}}"""
    category_context = PROMPT_CATEGORY_CONTEXT
    focus_context = (
        f"–§–æ–∫—É—Å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {focus_category_list}"
        if focus_category_list else ""
    )
    prompt_id = os.getenv("OPENAI_PROMPT_ID") or os.getenv("OPENAI_PROMPT_TEMPLATE_ID")
    prompt_version = os.getenv("OPENAI_PROMPT_VERSION")
    message_only = text
    user_prompt_parts = [category_context]
    if focus_context:
        user_prompt_parts.append(focus_context)
    user_prompt_parts.append(message_only)
    user_prompt = "\n\n".join(part for part in user_prompt_parts if part)
    prompt_payload = NOT_GIVEN
    instructions_arg = NOT_GIVEN
    prompt_variables = None
    prompt_cache_source = None
    raw_prompt_system = None
    if prompt_id:
        prompt_payload = {"id": prompt_id}
        if prompt_version:
            prompt_payload["version"] = prompt_version
        prompt_variables = {
            "text": message_only,
            "message": message_only,
            "user_prompt": user_prompt,
            "category_context": category_context,
            "category_list": category_list,
            "focus_category_list": focus_category_list,
            "focus_context": focus_context,
        }
        prompt_variables = {k: v for k, v in prompt_variables.items() if v}
        if prompt_variables:
            prompt_payload["variables"] = prompt_variables
        raw_prompt_system = f"prompt_id:{prompt_id}{'@'+prompt_version if prompt_version else ''}"
    else:
        instructions_arg = system_prompt
        raw_prompt_system = system_prompt
    input_messages = [
        {
            "role": "user",
            "content": [
                {"type": "input_text", "text": user_prompt}
            ],
        }
    ]
    if prompt_id:
        cache_components = [f"prompt:{prompt_id}:{prompt_version or 'latest'}"]
        if prompt_variables:
            stable_cache_vars = {
                key: prompt_variables.get(key)
                for key in ("category_context", "category_list")
                if prompt_variables.get(key)
            }
            if stable_cache_vars:
                cache_components.append(json.dumps(stable_cache_vars, ensure_ascii=False, sort_keys=True))
        prompt_cache_source = "|".join(cache_components)
    else:
        prompt_cache_source = system_prompt
    if not prompt_cache_source:
        prompt_cache_source = user_prompt
    # Store raw prompt for debug/tracing
    raw_prompt = {
        "system": raw_prompt_system,
        "user": user_prompt,
    }
    if prompt_variables:
        raw_prompt["prompt_variables"] = prompt_variables
    # Responses API: strict JSON schema via text.format
    model_name = os.getenv("OPENAI_MODEL", "gpt-5-nano")
    reasoning_effort = os.getenv("REASONING_EFFORT", "minimal")
    text_block = {
        "format": {
            "type": "json_schema",
            "name": "lead_classification",
            "strict": True,
            "schema": {
                "type": "object",
                "properties": {
                    "relevant": {"type": "boolean"},
                    "category": {"type": ["string", "null"]},
                    "subcategory": {"type": ["string", "null"]},
                    "region": {"type": ["string", "null"]},
                    "explanation": {"type": "string"},
                    "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                },
                "required": ["relevant", "category", "subcategory", "region", "explanation", "confidence"],
                "additionalProperties": False
            }
        }
    }
    verbosity = os.getenv("AI_VERBOSITY")
    if verbosity in ("low", "medium", "high"):
        text_block["verbosity"] = verbosity
    # Prompt caching key (stable for identical instructions setup)
    prompt_cache_key = hashlib.sha1(str(prompt_cache_source).encode("utf-8")).hexdigest()
    cache_enabled = os.getenv("ENABLE_PROMPT_CACHE", "1") == "1"
    cache_kwargs = {}
    if cache_enabled:
        cache_kwargs = {
            "prompt_cache_key": f"leadbot:{prompt_cache_key}",
        }
    response_kwargs = {
        "model": model_name,
        "input": input_messages,
        "reasoning": {"effort": reasoning_effort},
        "text": text_block,
        "store": True,
        "metadata": {"app": "leadbot", "component": "classifier"},
        **cache_kwargs,
    }
    if prompt_payload is not NOT_GIVEN:
        response_kwargs["prompt"] = prompt_payload
    if instructions_arg is not NOT_GIVEN:
        response_kwargs["instructions"] = instructions_arg
    try:
        resp = _responses_create_with_retry(
            client_ai,
            **response_kwargs,
        )
    except Exception as e:
        logger.warning("RESPONSES_CALL_FAILED: %s", e, exc_info=True)
        # Fallback 1: Chat Completions with response_format json_object
        try:
            _apply_rate_limit()
            fallback_user = user_prompt
            cc = client_ai.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": fallback_user},
                ],
                response_format={"type": "json_object"},
            )
            content = None
            try:
                content = cc.choices[0].message.content if getattr(cc.choices[0].message, 'content', None) else cc.choices[0].message["content"]
            except Exception:
                # try alternative attribute structure
                content = str(cc)
            parsed = _try_parse_ai_json(content or "")
            if not parsed:
                raise RuntimeError("ChatCompletions returned non-JSON content")
            _log_usage(model_name, cc, getattr(cc, 'id', None))
            result = _sanitize_result(parsed)
            raw_conf = result.get("confidence", 0.0)
            result["confidence"] = calibrate_confidence(raw_conf)
            result = _sanitize_result(result)
            # Cache and return
            if len(_classify_cache) >= _CLASSIFY_CACHE_MAXSIZE:
                _classify_cache.pop(next(iter(_classify_cache)))
            _classify_cache[key] = {"ts": time.time(), "payload": result.copy()}
            return result
        except Exception as e2:
            # Fallback 2: plain parsing failure
            return {
                "relevant": False,
                "category": None,
                "subcategory": None,
                "region": None,
                "explanation": f"OpenAI error: {e} | fallback: {e2}",
                "confidence": 0.0,
                "accepted": False,
            }
    # Token usage dedicated log
    _log_usage(model_name, resp, getattr(resp, 'id', None))
    # Prefer consolidated output_text helper, fallback to joining output items
    content = getattr(resp, 'output_text', None) or ""
    if not content:
        parts = []
        out = getattr(resp, 'output', None) or []
        try:
            for item in out:
                cont = getattr(item, 'content', None)
                if isinstance(cont, list):
                    for c in cont:
                        txt = getattr(c, 'text', None) if hasattr(c, 'text') else (c.get('text') if isinstance(c, dict) else None)
                        if txt:
                            parts.append(str(txt))
        except Exception:
            pass
        content = "\n".join(parts).strip()
    raw_model_output = content
    result = _try_parse_ai_json(content)
    if result is None:
        # Fallback: return safe default with raw included for trace
        return {
            "relevant": False,
            "category": None,
            "subcategory": None,
            "region": None,
            "explanation": "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ –ò–ò",
            "confidence": 0.0,
            "raw": content,
            "accepted": False,
            "raw_prompt": raw_prompt,
            "raw_model_output": raw_model_output,
        }
    # Sanitize, calibrate, and sanitize again (single clear flow)
    result = _sanitize_result(result)
    raw_conf = result.get("confidence", 0.0)
    result["confidence"] = calibrate_confidence(raw_conf)
    result = _sanitize_result(result)
    cat_name = result.get("category")
    if cat_name:
        normalized = CATEGORY_ALIASES.get(str(cat_name).lower())
        if normalized:
            result["category"] = normalized
    # Attach prompt and model output for trace/debug only if DEBUG_PROMPT_TRACE
    if DEBUG_PROMPT_TRACE:
        result["raw_prompt"] = raw_prompt
        result["raw_model_output"] = raw_model_output
    # –ö–µ—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–∫–æ–ø–∏—é) –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
    if len(_classify_cache) >= _CLASSIFY_CACHE_MAXSIZE:
        _classify_cache.pop(next(iter(_classify_cache)))
    _classify_cache[key] = {"ts": time.time(), "payload": result.copy()}
    return result

# --- apply_overrides and helpers moved from Botparsing.py ---


def apply_overrides(cla, lower_text, category_heuristic):
    """
    Post-hoc override logic for classification adjustment (heuristics only).
    """
    lower = lower_text
    # –Ø–≤–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∞—Ä–µ–Ω–¥—ã
    rental_signals = [
        "—Å–Ω–∏–º—É", "—Å–Ω—è—Ç—å",
        "–∏—â—É –∫–≤–∞—Ä—Ç–∏—Ä—É", "–∏—â–µ–º –∫–≤–∞—Ä—Ç–∏—Ä—É", "–∏—â–µ—Ç –∫–≤–∞—Ä—Ç–∏—Ä—É",
        "–∫–æ—Ä–æ—Ç–∫–∏–π —Å—Ä–æ–∫", "–Ω–∞ –º–µ—Å—è—Ü", "–Ω–∞ 1 –º–µ—Å—è—Ü", "–Ω–∞ –æ–¥–∏–Ω –º–µ—Å—è—Ü"
    ]
    if any(w in lower for w in rental_signals) or ("–∫–≤–∞—Ä—Ç–∏—Ä" in lower and any(w in lower for w in ["–∏—â—É", "–∏—â–µ–º", "–∏—â–µ—Ç"])):
        cla["relevant"] = True
        cla["category"] = cla.get("category") or "–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å"
        cla["explanation"] = "–ó–∞–ø—Ä–æ—Å –∞—Ä–µ–Ω–¥—ã –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"
        return cla
    # –î–æ–º–µ–Ω–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ: –º–∞—Å—Å–∞–∂ ‚Üí –±—å—é—Ç–∏
    if any(w in lower for w in ("–º–∞—Å—Å–∞–∂", "–º–∞—Å—Å–∞–∂–∏—Å—Ç", "–º–∞—Å—Å–∞–∂–∏—Å—Ç–∫–∞")):
        cla["category"] = "–±—å—é—Ç–∏"
    # –†–µ–∫–ª–∞–º–∞ —Å—Ç—Ä–∞—Ö–æ–≤–∫–∏: salesy –±–µ–∑ –≤–æ–ø—Ä–æ—Å–∞
    if "—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞" in lower:
        salesy = any(w in lower for w in SALESY_TERMS)
        if salesy and not any(q in lower for q in QUESTION_INDICATORS):
            cla["relevant"] = False
            cla["explanation"] = "–†–µ–∫–ª–∞–º–∞ —Å—Ç—Ä–∞—Ö–æ–≤–∫–∏"
            return cla  # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É, –µ—Å–ª–∏ —Ç–æ—á–Ω–æ —Ä–µ–∫–ª–∞–º–∞
    # 2.1) –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ—Ç—Å–µ—á–∫–∞ –ª–∏—Å—Ç–∏–Ω–≥–æ–≤ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –ò–ò
    # –ï—Å–ª–∏ –ò–ò (–∏–ª–∏ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞) –≤—ã–±—Ä–∞–ª ¬´–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å¬ª, –Ω–æ —Ç–µ–∫—Å—Ç –ø–æ—Ö–æ–∂ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–≤—Ü–∞ ‚Äî –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ
    if (cla.get("category") == "–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å" or category_heuristic == "–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å"):
        has_seller = any(t in lower for t in SELLER_TERMS)
        price_matches = _PRICE_REGEX.findall(lower)
        has_price = bool(price_matches)
        has_layout = bool(re.search(r"\b[1-5]\s*([+x—Ö])\s*[0-5]\b", lower))
        has_area = bool(re.search(r"\b(–∫–≤\.?\s?–º|–º2|–º\^2|–º¬≤|sqm|sq\s?m)\b", lower))
        has_realty_hint = any(tok in lower for tok in REALTY_HINT_TERMS)
        has_buyer = any(bt in lower for bt in BUYER_TRIGGERS)
        sellerish = has_seller or has_price or has_layout or has_area or has_realty_hint
        # –µ—Å–ª–∏ –ø—Ä–æ–¥–∞–≤–µ—Ü –∏ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ –ø–æ–∫—É–ø–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ ‚Äî –Ω–µ –ª–∏–¥
        emoji_sections = lower.count("üåü") + lower.count("üå¥") + lower.count("‚ú®")
        promo_cta_hits = sum(lower.count(term) for term in PROMO_CTA_TERMS)
        if sellerish and not has_buyer:
            cla["relevant"] = False
            cla["explanation"] = "–†–∏—ç–ª—Ç–æ—Ä—Å–∫–∏–π –ª–∏—Å—Ç–∏–Ω–≥/–ø—Ä–æ–¥–∞–∂–∞"
            return cla  # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É, –µ—Å–ª–∏ —Ç–æ—á–Ω–æ —Ä–µ–∫–ª–∞–º–∞
        if (len(price_matches) >= 2 or emoji_sections >= 2 or promo_cta_hits >= 2) and not has_buyer:
            cla["relevant"] = False
            cla["explanation"] = "–†–∏—ç–ª—Ç–æ—Ä—Å–∫–∏–π –ª–∏—Å—Ç–∏–Ω–≥/–ø—Ä–æ–¥–∞–∂–∞"
            return cla
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —É—Å–ª—É–≥ (–Ω–µ –∑–∞–ø—Ä–æ—Å–æ–≤)
    has_offer = any(term in lower for term in OFFER_TERMS)
    has_buyer_request = any(trigger in lower for trigger in BUYER_TRIGGERS)
    if has_offer and not has_buyer_request:
        cla["relevant"] = False
        cla["explanation"] = "–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —É—Å–ª—É–≥, –∞ –Ω–µ –∑–∞–ø—Ä–æ—Å"
        return cla  # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É, –µ—Å–ª–∏ —Ç–æ—á–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤
    review_terms = [
        "–æ—Ç–ª–∏—á–Ω–æ", "—Ö–æ—Ä–æ—à–æ", "–ø–ª–æ—Ö–æ", "—É–∂–∞—Å–Ω–æ", "–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é", "—Ä–µ–∫–æ–º–µ–Ω–¥—É—é", "—Å–æ–≤–µ—Ç—É—é", 
        "–Ω–µ —Å–æ–≤–µ—Ç—É—é", "–æ–ø—ã—Ç", "—Ä–∞–±–æ—Ç–∞–ª", "—Ä–∞–±–æ—Ç–∞–ª–∞", "–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è", "–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å"
    ]
    has_review = any(term in lower for term in review_terms)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    recommendation_request_terms = ["–ø–æ—Å–æ–≤–µ—Ç—É–π", "–ø–æ—Å–æ–≤–µ—Ç—É–π—Ç–µ", "–∫—Ç–æ –∑–Ω–∞–µ—Ç", "–∫—Ç–æ –º–æ–∂–µ—Ç", "–∫—Ç–æ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è"]
    is_recommendation_request = any(term in lower for term in recommendation_request_terms)
    
    if has_review and not has_buyer_request and not is_recommendation_request:
        cla["relevant"] = False
        cla["explanation"] = "–û—Ç–∑—ã–≤ –∏–ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π, –∞ –Ω–µ –∑–∞–ø—Ä–æ—Å"
        return cla  # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É, –µ—Å–ª–∏ —Ç–æ—á–Ω–æ –æ—Ç–∑—ã–≤
    # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ù–ï –¥–æ–ª–∂–Ω–∞ –∂—ë—Å—Ç–∫–æ –ø–µ—Ä–µ–±–∏–≤–∞—Ç—å –ò–ò:
    # –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É –ò–ò –Ω–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è
    if category_heuristic:
        ai_cat = cla.get("category")
        try:
            ai_conf = float(cla.get("confidence", 0.0) or 0.0)
        except Exception:
            ai_conf = 0.0
        if not ai_cat or ai_conf < 0.6:
            cla["category"] = category_heuristic
    # –ú—è–≥–∫–∞—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ confidence –ø—Ä–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏ —Å —ç–≤—Ä–∏—Å—Ç–∏–∫–æ–π
    conf = cla.get("confidence", 0.0)
    if category_heuristic and cla.get("category") == category_heuristic and conf < 0.8:
        cla["confidence"] = min(0.85, conf + 0.1)
    return cla


def _stem_in_text(needle: str, stems_set: set[str]) -> bool:
    """True if stem(needle) –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ (–ø–æ —Å—Ç–µ–º–∞–º)."""
    return _stem(needle) in stems_set

# --- Exports ---------------------------------------------------------------
__all__ = [
    "classify_text_with_ai",
    "apply_overrides",
    "_classify_cache",
    "classify_relevance",
]# ai_utils.py (–≤ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞)


def update_categories():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –ø–∞–º—è—Ç–∏ –∏–∑ —Ñ–∞–π–ª–∞ categories.json"""
    global categories
    try:
        with open("categories.json", "r", encoding="utf-8") as f:
            categories = json.load(f)
        print("‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏–∑ categories.json")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")

# --- Backward-compat cache helpers for tests/tools ---


def _norm_text(s: str) -> str:
    """Normalize text for cache key creation (lowercase + strip)."""
    return (s or "").strip().lower()


def _cache_put(key: str, value: dict):
    """Put value into classification cache with current timestamp."""
    if len(_classify_cache) >= _CLASSIFY_CACHE_MAXSIZE:
        _classify_cache.pop(next(iter(_classify_cache)))
    _classify_cache[key] = {"ts": time.time(), "payload": value.copy() if isinstance(value, dict) else value}


def _cache_get(key: str):
    """Get cached value if present and not expired; else None."""
    entry = _classify_cache.get(key)
    if not isinstance(entry, dict):
        return None
    ts = entry.get("ts")
    payload = entry.get("payload")
    if ts is None or (time.time() - ts) > _CLASSIFY_CACHE_TTL:
        try:
            del _classify_cache[key]
        except KeyError:
            pass
        return None
    return payload
