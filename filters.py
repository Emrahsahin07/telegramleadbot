from __future__ import annotations
r"""
filters.py
–°–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ª–∏–¥‚Äë—Å–æ–æ–±—â–µ–Ω–∏–π.

–§—É–Ω–∫—Ü–∏–∏:
‚Ä¢ extract_stems(entry)        ‚Äì —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ keywords –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–π/–ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π.
‚Ä¢ is_similar(a, b, threshold) ‚Äì –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —á–µ—Ä–µ–∑ RapidFuzz.
‚Ä¢ contains_negative(text)     ‚Äì –¥–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç —Å–ª–æ–≤–∞‚Äë—Ç—Ä–∏–≥–≥–µ—Ä—ã ¬´–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ, —Å–ø–∞–º, –º–æ—à–µ–Ω–Ω–∏–∫‚Ä¶¬ª,
                                –∏–≥–Ω–æ—Ä–∏—Ä—É—è ¬´–Ω–µ —Å–ø–∞–º¬ª –±–ª–∞–≥–æ–¥–∞—Ä—è –æ—Ç—Ä–∏—Ü–∞–Ω–∏—é (?<!–Ω–µ\s).
"""

from typing import Any, List, Union
import re
from config import LOCATION_ALIAS
from constants import BUYER_TRIGGERS, SELLER_TERMS, OFFER_TERMS, REALTY_HINT_TERMS, PROMO_CTA_TERMS
from rapidfuzz import fuzz
import logging

# ---------- Contact detection ---------------------------------------------
_contact_regex = re.compile(
    r"(@[\w_]+|t\.me/[\w_\-]+|https?://t\.me/[\w_\-]+|\+?[\d\-\s\(\)]{7,}|whatsapp)",
    flags=re.IGNORECASE
)

def contains_contact(text: str) -> bool:
    """
    True if text contains contact info: @username, t.me links, phone numbers, or whatsapp.
    """
    return bool(_contact_regex.search(text))


# ---------- Negative context -----------------------------------------------
NEGATIVE_STEMS: List[str] = [
    "–æ—Å—Ç–æ—Ä–æ–∂–Ω",
    "–º–æ—à–µ–Ω–Ω–∏–∫",
    "—Å–ø–∞–º",
    "—Ä–µ–∫–ª–∞–º–∞",
    "–º–æ—à–µ–Ω–Ω–∏–∫–æ–≤",
    "–ø—Ä–µ–¥–æ–ø–ª–∞—Ç",
    "–æ–±–º–∞–Ω",
    "–∫–∏–¥–∞–ª–∏",
    "–∫–∏–¥–∞—é—Ç",
    "–Ω–µ —Å–æ–≤–µ—Ç—É—é",
    "—Ñ–µ–π–∫",
    "–¥–µ—à–µ–≤–æ –æ—Ç–∫—Ä–æ–≤–µ–Ω–Ω–æ",
]

NEGATIVE_PHRASES: List[str] = [
    "–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é",
    "–æ—Å—Ç–µ—Ä–µ–≥–∞–π—Ç–µ—Å—å",
    "–±–µ—Ä–µ–≥–∏—Ç–µ—Å—å",
    "–Ω–µ —Å—Ç–æ–∏—Ç",
    "—Ä–∞–∑–≤–æ–¥",
    "–æ—Ç—Å—Ç–æ–π",
    "–ø–ª–æ—Ö–æ–π —Å–µ—Ä–≤–∏—Å",
    "–Ω–∏–∫–æ–º—É –Ω–µ —Å–æ–≤–µ—Ç—É—é"
]

_NEGATIVE_REGEX = re.compile(
    rf"(?<!\b–Ω–µ\s)({'|'.join(NEGATIVE_STEMS)})",
    flags=re.IGNORECASE
)

def contains_negative(text: str) -> bool:
    """True, –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç."""
    text_lower = text.lower()
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
    if _NEGATIVE_REGEX.search(text_lower):
        return True
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ñ—Ä–∞–∑—ã
    if any(phrase in text_lower for phrase in NEGATIVE_PHRASES):
        return True
    return False


# ---------- Fuzzy similarity ------------------------------------------------
def is_similar(a: str, b: str, threshold: int = 70) -> bool:
    """
    –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ —Å –ø–æ–º–æ—â—å—é RapidFuzz.
    threshold ‚Äì –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (0‚Äì100).
    """
    return fuzz.partial_ratio(a.lower(), b.lower()) >= threshold


# ---------- Keyword stems extraction ---------------------------------------
def extract_stems(entry: Any) -> List[str]:
    """
    –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Å—Ç—Ä–æ–∫–∏‚Äë–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑
    —Å–ª–æ–≤–∞—Ä—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π/–ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–π:

    {
        "keywords": [...],
        "subcategories": {
            "–ø–æ–¥–∫–∞—Ç": { "keywords": [...] }
        }
    }
    """
    stems: List[str] = []
    if isinstance(entry, dict):
        if "keywords" in entry and isinstance(entry["keywords"], list):
            stems.extend(entry["keywords"])
        # —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –≤–≥–ª—É–±—å –≤—Å–µ—Ö –∫–ª—é—á–µ–π, –∫—Ä–æ–º–µ keywords
        for key, val in entry.items():
            if key != "keywords":
                stems.extend(extract_stems(val))
    elif isinstance(entry, list):
        for item in entry:
            stems.extend(extract_stems(item))
    elif isinstance(entry, str):
        stems.append(entry)
    return stems


# --- Word boundary matching -----------------------------------------------
def _contains_word(haystack: str, needle: str) -> bool:
    """
    True if `needle` appears in `haystack` as a separate word/morpheme,
    not as part of another word.
    """
    pattern = rf"(?<![–∞-—èa-z—ë]){re.escape(needle)}(?![–∞-—èa-z—ë])"
    return bool(re.search(pattern, haystack, flags=re.IGNORECASE))

# ---------- Advertisement detection ---------------------------------------
_PRICE_REGEX = re.compile(
    r"\b\d+(?:[\.,]\d+)?\s?(?:‚Ç¨|eur|–µ–≤—Ä–æ|usd|\$|–¥–æ–ª–ª–∞—Ä|‚Ç∫|try)(?:\b|[/\-]?[–∞-—èa-z]{0,8})",
    flags=re.IGNORECASE
)

_AD_PATTERNS = [
    r"(\+?\d[\d\-\s\(\)]{6,}\d)",       # —Ç–µ–ª–µ—Ñ–æ–Ω
    _PRICE_REGEX,                                # —Ü–µ–Ω–∞ + –≤–∞–ª—é—Ç–∞ (–≤–∫–ª—é—á–∞—è ‚Ç¨/—Å—É—Ç–∫–∏ –∏ —Ç.–ø.)
]

_SELLER_TERMS = SELLER_TERMS

# –†–µ–∞–ª–µ—Å—Ç–µ–π—Ç-—Ö–∏–Ω—Ç—ã –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–ª–∞–Ω–∏—Ä–æ–≤–æ–∫/–ø–ª–æ—â–∞–¥–∏
_REALTY_HINTS = REALTY_HINT_TERMS

_LAYOUT_REGEX = re.compile(r"\b[1-5]\s*([+x—Ö])\s*[0-5]\b")
_AREA_REGEX = re.compile(r"\b(–∫–≤\.?\s?–º|–º2|–º\^2|–º¬≤|sqm|sq\s?m)\b", flags=re.IGNORECASE)

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
_OFFER_TERMS = OFFER_TERMS

_REVIEW_TERMS = [
    "–æ—Ç–ª–∏—á–Ω–æ", "—Ö–æ—Ä–æ—à–æ", "–ø–ª–æ—Ö–æ", "—É–∂–∞—Å–Ω–æ", "–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é", "—Ä–µ–∫–æ–º–µ–Ω–¥—É—é", "—Å–æ–≤–µ—Ç—É—é", 
    "–Ω–µ —Å–æ–≤–µ—Ç—É—é", "–æ–ø—ã—Ç", "—Ä–∞–±–æ—Ç–∞–ª", "—Ä–∞–±–æ—Ç–∞–ª–∞", "–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è", "–ø–æ–ª—å–∑–æ–≤–∞–ª–∞—Å—å"
]

def is_advertisement(text: str) -> bool:
    """
    True –µ—Å–ª–∏ –ø–æ—Ö–æ–∂–µ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –æ –ø—Ä–æ–¥–∞–∂–µ/–∞—Ä–µ–Ω–¥–µ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏:
    –∫–æ–Ω—Ç–∞–∫—Ç + (—Ç–µ—Ä–º–∏–Ω—ã –ø—Ä–æ–¥–∞–≤—Ü–∞ | —Ü–µ–Ω–∞/–≤–∞–ª—é—Ç–∞ | –º–Ω–æ–≥–æ —Ö—ç—à—Ç–µ–≥–æ–≤), –ª–∏–±–æ —è–≤–Ω—ã–µ —Ü–µ–Ω–Ω–∏–∫–∏/—Ç–µ—Ä–º–∏–Ω—ã.
    –ù–µ –ø–æ–º–µ—á–∞–µ—Ç –∫–∞–∫ —Ä–µ–∫–ª–∞–º—É –ø—Ä–æ—Å—Ç–æ –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–∞ –±–µ–∑ –ø—Ä–æ–¥–∞–∂–∏.
    """
    text_low = text.lower()
    has_contact = contains_contact(text_low)
    has_price_or_currency = any(
        (pat.search(text_low) if hasattr(pat, "search") else re.search(pat, text_low))
        for pat in _AD_PATTERNS
    )
    has_seller_terms = any(term in text_low for term in _SELLER_TERMS)
    has_layout = bool(_LAYOUT_REGEX.search(text_low))
    has_area = bool(_AREA_REGEX.search(text_low))
    has_realty_hint = any(tok in text_low for tok in _REALTY_HINTS)
    many_hashtags = len(re.findall(r"#\w+", text_low)) > 3
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
    has_offer_terms = any(term in text_low for term in _OFFER_TERMS)
    has_review_terms = any(term in text_low for term in _REVIEW_TERMS)

    # –ü–æ–∫—É–ø–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã ‚Äî –Ω–µ —Å—á–∏—Ç–∞–µ–º —Ä–µ–∫–ª–∞–º–æ–π. –†–∞–∑—Ä–µ—à–∞–µ–º —Ü–µ–Ω—É, –µ—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω—ã—Ö seller-—Ç–µ—Ä–º–∏–Ω–æ–≤
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É—á–∏—Ç—ã–≤–∞–µ–º –º–∞—Ä–∫–µ—Ä—ã –±—é–¥–∂–µ—Ç–∞ (–±—é–¥–∂–µ—Ç/–¥–æ + —Ü–µ–Ω–∞) –∫–∞–∫ —Å–ø—Ä–æ—Å
    has_buyer = any(bt in text_low for bt in BUYER_TRIGGERS)
    has_budget_marker = bool(re.search(r"\b(–±—é–¥–∂–µ—Ç|–¥–æ)\b", text_low)) and has_price_or_currency
    if (has_buyer or has_budget_marker) and not has_seller_terms:
        return False

    sellerish = has_seller_terms or has_price_or_currency or has_layout or has_area or has_realty_hint

    price_hits = len(_PRICE_REGEX.findall(text_low))
    emoji_sections = text_low.count("üåü") + text_low.count("üå¥") + text_low.count("‚ú®")
    promo_cta_hits = sum(text_low.count(term) for term in PROMO_CTA_TERMS)

    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –æ—Ç—Å–µ—á–µ–Ω–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏–π
    if (has_contact and (sellerish or many_hashtags)):
        return True
    if has_price_or_currency and (sellerish or has_contact or many_hashtags):
        return True
    if has_seller_terms and (has_contact or has_price_or_currency or many_hashtags or has_realty_hint):
        return True

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã –º–Ω–æ–≥–æ–æ–±—ä–µ–∫—Ç–Ω—ã—Ö –ª–∏—Å—Ç–∏–Ω–≥–æ–≤
    if (price_hits >= 2 or emoji_sections >= 2 or promo_cta_hits >= 2) and not has_buyer:
        return True

    # –î–ª–∏–Ω–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è-–ª–∏—Å—Ç–∏–Ω–≥–∏ –±–µ–∑ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ ‚Äî –ø–æ—á—Ç–∏ –Ω–∞–≤–µ—Ä–Ω—è–∫–∞ —Ä–µ–∫–ª–∞–º–∞
    question_triggers = ["?", "–ø–æ–¥—Å–∫–∞–∂–∏—Ç–µ", "—Å–∫–æ–ª—å–∫–æ", "–≥–¥–µ", "–∫—Ç–æ –º–æ–∂–µ—Ç", "–Ω—É–∂–µ–Ω", "–∏—â—É", "–Ω—É–∂–Ω–∞", "—Ç—Ä–µ–±—É–µ—Ç—Å—è"]
    if len(text_low) > 220 and sellerish and not any(q in text_low for q in question_triggers):
        return True
        
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —É—Å–ª—É–≥ (–Ω–µ –∑–∞–ø—Ä–æ—Å–æ–≤)
    if has_offer_terms and not any(bt in text_low for bt in BUYER_TRIGGERS):
        return True
        
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤
    if has_review_terms and not any(bt in text_low for bt in BUYER_TRIGGERS):
        return True

    return False


# ---------------- Region & route utilities (moved from Botparsing.py) ----------------
# City keywords to canonical names (english/ru variants)
CITY_KEYWORDS = {
    "antalya": "–ê–Ω—Ç–∞–ª–∏—è", "–∞–Ω—Ç–∞–ª–∏—è": "–ê–Ω—Ç–∞–ª–∏—è",
    "alanya": "–ê–ª–∞–Ω—å—è", "–∞–ª–∞–Ω–∏—è": "–ê–ª–∞–Ω—å—è",
    "kemer": "–ö–µ–º–µ—Ä", "–∫–µ–º–µ—Ä": "–ö–µ–º–µ—Ä",
    "belek": "–ë–µ–ª–µ–∫", "–±–µ–ª–µ–∫": "–ë–µ–ª–µ–∫",
    "side": "–°–∏–¥–µ", "—Å–∏–¥–µ": "–°–∏–¥–µ",
    "istanbul": "–°—Ç–∞–º–±—É–ª", "–∏—Å—Ç–∞–º–±—É–ª": "–°—Ç–∞–º–±—É–ª", "—Å—Ç–∞–º–±—É–ª": "–°—Ç–∞–º–±—É–ª",
    "kundu": "–ö—É–Ω–¥—É", "–∫—É–Ω–¥—É": "–ö—É–Ω–¥—É",
    "fethiye": "–§–µ—Ç—Ö–∏–µ", "—Ñ–µ—Ç—Ö–∏–µ": "–§–µ—Ç—Ö–∏–µ",
    "mersin": "–ú–µ—Ä—Å–∏–Ω", "–º–µ—Ä—Å–∏–Ω": "–ú–µ—Ä—Å–∏–Ω",
    "beldibi": "–ë–µ–ª—å–¥–∏–±–∏", "–±–µ–ª—å–¥–∏–±–∏": "–ë–µ–ª—å–¥–∏–±–∏",
    "goynuk": "–ì—ë–π–Ω—é–∫","–≥–µ–π–Ω—é–∫": "–ì—ë–π–Ω—é–∫", "g√∂yn√ºk": "–ì—ë–π–Ω—é–∫"
}

# Merge LOCATION_ALIAS with CITY_KEYWORDS to widen coverage (e.g., –§–µ—Ç—Ö–∏–µ/Fethiye)
MERGED_ALIASES = dict(LOCATION_ALIAS)
for k, v in CITY_KEYWORDS.items():
    MERGED_ALIASES[k] = v


def _alias_regex(alias: str) -> str:
    """Regex for alias tolerant to Russian endings. Latin remains strict."""
    alias = alias.lower()
    base = re.escape(alias)
    if re.search(r"[a-z]", alias):
        return base
    return base + r"[–∞-—è—ë]*"

# Airports
AIRPORT_CODES = {
    "ayt": "–ê–Ω—Ç–∞–ª–∏—è",  # Antalya airport
    "ist": "–°—Ç–∞–º–±—É–ª",  # Istanbul new airport
    "saw": "–°—Ç–∞–º–±—É–ª",  # Sabiha G√∂k√ßen
}


def _all_locations_from_text(text_lower: str):
    locs = set()
    for alias, canon in MERGED_ALIASES.items():
        pat = _alias_regex(alias)
        if re.search(rf"(?<![–∞-—èa-z—ë]){pat}(?![–∞-—èa-z—ë])", text_lower):
            locs.add(canon)
    for code, canon in AIRPORT_CODES.items():
        if re.search(rf"\b{code}\b", text_lower):
            locs.add(canon)
    return list(locs)


def infer_region_from_text(title: str, username: str, text_lower: str):
    title_l = (title or "").lower()
    uname_l = (username or "").lower()

    # 1) Try to match from chat title first (strict word boundaries)
    for alias, canon in LOCATION_ALIAS.items():
        pat = _alias_regex(alias)
        if re.search(rf"(?<![–∞-—èa-z—ë]){pat}(?![–∞-—èa-z—ë])", title_l):
            return canon

    # 2) Then from message text (strict)
    for alias, canon in LOCATION_ALIAS.items():
        pat = _alias_regex(alias)
        if re.search(rf"(?<![–∞-—èa-z—ë]){pat}(?![–∞-—èa-z—ë])", text_lower):
            return canon

    # 3) Fall back to city keywords (title/username only, strict)
    for key, canon in CITY_KEYWORDS.items():
        pat = _alias_regex(key)
        if re.search(rf"(?<![–∞-—èa-z—ë]){pat}(?![–∞-—èa-z—ë])", title_l) or \
           re.search(rf"(?<![–∞-—èa-z—ë]){pat}(?![–∞-—èa-z—ë])", uname_l):
            return canon
    return None


def extract_transfer_route(text_lower: str, region_chat: Union[str, None]):
    """Return (pickup, destination) for transfer-like requests.
    Heuristics:
      - pickup: after '–∏–∑|—Å|–æ—Ç', '–∏–∑ –∞—ç—Ä–æ–ø–æ—Ä—Ç–∞', airport codes
      - destination: after '–≤|–¥–æ|–∫'
      - if only destination given ‚Üí pickup = region_chat
      - if only pickup given ‚Üí destination = None
      - if nothing found and region_chat is None ‚Üí pickup=None
    """
    pickup = None
    destination = None

    # prepositions
    for alias, canon in MERGED_ALIASES.items():
        pat = _alias_regex(alias)
        if re.search(rf"\b(–∏–∑|—Å|–æ—Ç)\s+{pat}\b", text_lower):
            pickup = canon
            break
    for alias, canon in MERGED_ALIASES.items():
        pat = _alias_regex(alias)
        if re.search(rf"\b(–≤|–¥–æ|–∫)\s+{pat}\b", text_lower):
            destination = canon
            break

    # airports (prefer region_chat unless explicit code present)
    if re.search(r"\b–≤\s+–∞—ç—Ä–æ–ø–æ—Ä—Ç[–∞-—è—ë]*\b", text_lower):
        if destination is None:
            for code, canon in AIRPORT_CODES.items():
                if re.search(rf"\b{code}\b", text_lower):
                    destination = canon
                    break
            if destination is None:
                destination = region_chat
    if re.search(r"\b–∏–∑\s+–∞—ç—Ä–æ–ø–æ—Ä—Ç[–∞-—è—ë]*\b", text_lower):
        if pickup is None:
            for code, canon in AIRPORT_CODES.items():
                if re.search(rf"\b{code}\b", text_lower):
                    pickup = canon
                    break
            if pickup is None:
                pickup = region_chat

    # fallback destination from any loose location other than pickup
    if destination is None:
        for loc in _all_locations_from_text(text_lower):
            if loc != pickup:
                destination = loc
                break

    if pickup is None and region_chat:
        pickup = region_chat
    return pickup, destination
